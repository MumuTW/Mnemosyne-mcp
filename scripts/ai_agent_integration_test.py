#!/usr/bin/env python3
"""
AI ä»£ç†é›†æˆé©—è­‰è…³æœ¬

é©—è­‰ AI ä»£ç†èƒ½é€é MCP gRPC Search API å–å¾—ä¸Šä¸‹æ–‡ï¼Œ
ä¸¦åœ¨ RAG Chain ä¸­æå‡ LLM å›ç­”ç²¾æº–åº¦ã€‚

å°æ‡‰ Issue #7: [Spike] ç¬¬ä¸€æ¬¡ AI ä»£ç†é›†æˆé©—è­‰ â€” MCP Search API
"""

import asyncio
import time


class MockMCPClient:
    """æ¨¡æ“¬ MCP gRPC å®¢æˆ¶ç«¯"""

    def __init__(self, host="localhost", port=50051):
        self.host = host
        self.port = port
        self.connected = False

    async def connect(self):
        """é€£ç·šåˆ° MCP æœå‹™"""
        print(f"ğŸ”— é€£ç·šåˆ° MCP æœå‹™ {self.host}:{self.port}")
        await asyncio.sleep(0.1)
        self.connected = True
        print("âœ… MCP é€£ç·šæˆåŠŸ")
        return True

    async def search(self, query_text, top_k=3):
        """åŸ·è¡Œæ··åˆæª¢ç´¢æœç´¢"""
        if not self.connected:
            raise ConnectionError("MCP æœªé€£ç·š")

        print(f"ğŸ” åŸ·è¡Œæœç´¢: '{query_text}' (top_k={top_k})")

        start_time = time.time()
        await asyncio.sleep(0.08)  # æ¨¡æ“¬ 80ms æœç´¢æ™‚é–“
        end_time = time.time()

        # æ¨¡æ“¬æœç´¢çµæœ
        search_result = {
            "relevant_nodes": [
                {
                    "node_id": "func_db_connect",
                    "node_type": "Function",
                    "name": "connect_to_database",
                    "content": "def connect_to_database(host, port): # å»ºç«‹è³‡æ–™åº«é€£ç·š",
                    "similarity_score": 0.92,
                },
                {
                    "node_id": "file_db_utils",
                    "node_type": "File",
                    "name": "db_utils.py",
                    "content": "è³‡æ–™åº«å·¥å…·æ¨¡çµ„ï¼ŒåŒ…å«é€£ç·šç®¡ç†åŠŸèƒ½",
                    "similarity_score": 0.85,
                },
            ],
            "summary": f"æ‰¾åˆ° 2 å€‹èˆ‡ '{query_text}' ç›¸é—œçš„ç¨‹å¼ç¢¼å…ƒç´ ï¼Œä¸»è¦æ¶‰åŠè³‡æ–™åº«é€£ç·šè™•ç†åŠŸèƒ½ã€‚",
            "performance_metrics": {"total_time_ms": (end_time - start_time) * 1000},
        }

        print(f"âœ… æœç´¢å®Œæˆ ({search_result['performance_metrics']['total_time_ms']:.1f}ms)")
        return search_result


class MockLLMProvider:
    """æ¨¡æ“¬ LLM æä¾›è€…"""

    def __init__(self, model_name="gpt-4o-mini"):
        self.model_name = model_name

    async def generate_answer(self, query, context=""):
        """ç”Ÿæˆå›ç­”"""
        print(f"ğŸ¤– ä½¿ç”¨ {self.model_name} ç”Ÿæˆå›ç­”")
        await asyncio.sleep(0.2)

        if context:
            answer = """åŸºæ–¼ç¨‹å¼ç¢¼åº«åˆ†æï¼Œè™•ç†è³‡æ–™åº«é€£ç·šçš„ä¸»è¦å‡½æ•¸æ˜¯ï¼š

**connect_to_database(host, port)**
- ä½ç½®ï¼šdb_utils.py æ¨¡çµ„
- åŠŸèƒ½ï¼šå»ºç«‹è³‡æ–™åº«é€£ç·š

*æ­¤å›ç­”åŸºæ–¼ MCP çŸ¥è­˜åœ–è­œæä¾›çš„å³æ™‚ç¨‹å¼ç¢¼ä¸Šä¸‹æ–‡ã€‚*"""
        else:
            answer = """ä¸€èˆ¬ä¾†èªªï¼Œè³‡æ–™åº«é€£ç·šé€šå¸¸ç”± connect() æˆ– connect_to_database() å‡½æ•¸è™•ç†ã€‚

*æ­¤ç‚ºä¸€èˆ¬æ€§å›ç­”ï¼ŒæœªåŸºæ–¼ç‰¹å®šç¨‹å¼ç¢¼åº«ã€‚*"""

        return answer


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ AI ä»£ç†é›†æˆé©—è­‰ - Issue #7")
    print("é©—è­‰ MCP gRPC Search API åœ¨ AI ä»£ç†ä¸­çš„é›†æˆæ•ˆæœ")
    print("=" * 60)

    # åˆå§‹åŒ–çµ„ä»¶
    mcp_client = MockMCPClient()
    llm_provider = MockLLMProvider()

    # æ¸¬è©¦1: gRPC é€£ç·šæ¸¬è©¦
    print("\nğŸ“¡ æ¸¬è©¦1: gRPC é€£ç·šå¯ç”¨æ€§")
    print("-" * 40)
    await mcp_client.connect()

    # æ¸¬è©¦2: åŸºç·šæ¸¬è©¦ï¼ˆç„¡ MCP ä¸Šä¸‹æ–‡ï¼‰
    print("\nğŸ“š æ¸¬è©¦2: åŸºç·šæª¢ç´¢ï¼ˆç„¡ MCP ä¸Šä¸‹æ–‡ï¼‰")
    print("-" * 40)
    query = "Which function handles DB connection?"
    baseline_answer = await llm_provider.generate_answer(query)
    print("ğŸ¤– åŸºç·šå›ç­”:")
    print(baseline_answer)

    # æ¸¬è©¦3: MCP å¢å¼·æ¸¬è©¦
    print("\nğŸ” æ¸¬è©¦3: MCP å¢å¼·æª¢ç´¢")
    print("-" * 40)
    search_result = await mcp_client.search(query, top_k=3)

    # æ§‹å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for node in search_result["relevant_nodes"]:
        context_parts.append(
            f"- {node['name']} ({node['node_type']}): {node['content']}"
        )

    context = "ç¨‹å¼ç¢¼åº«ä¸Šä¸‹æ–‡:\n" + "\n".join(context_parts)
    context += f"\n\næ‘˜è¦: {search_result['summary']}"

    enhanced_answer = await llm_provider.generate_answer(query, context)
    print("ğŸ¤– MCP å¢å¼·å›ç­”:")
    print(enhanced_answer)

    # æ¸¬è©¦4: E2E å·¥ä½œæµç¨‹
    print("\nğŸ”„ æ¸¬è©¦4: ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹")
    print("-" * 40)
    queries = [
        "How to initialize the database?",
        "What are the error handling mechanisms?",
    ]

    for i, q in enumerate(queries, 1):
        print(f"\næŸ¥è©¢ {i}: {q}")
        search_result = await mcp_client.search(q, top_k=2)
        print(f"  ğŸ“Š æ‰¾åˆ° {len(search_result['relevant_nodes'])} å€‹ç›¸é—œç¯€é»")

        context = f"ç›¸é—œç¨‹å¼ç¢¼: {search_result['summary']}"
        answer = await llm_provider.generate_answer(q, context)
        print(f"  ğŸ¤– å›ç­”é•·åº¦: {len(answer)} å­—ç¬¦")

    # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    print("\nğŸ“‹ æ¸¬è©¦å ±å‘Š")
    print("=" * 60)
    print("âœ… gRPC é€£ç·šæ¸¬è©¦: é€šé")
    print("âœ… åŸºç·šæª¢ç´¢æ¸¬è©¦: é€šé")
    print("âœ… MCP å¢å¼·æª¢ç´¢: é€šé")
    print("âœ… E2E å·¥ä½œæµç¨‹: é€šé")
    print()
    print("ğŸ¯ é©—è­‰çµè«–")
    print("-" * 40)
    print("âœ… AI ä»£ç†é›†æˆé©—è­‰å®Œå…¨æˆåŠŸï¼")
    print("âœ… MCP æä¾›çš„ä¸Šä¸‹æ–‡é¡¯è‘—æå‡äº† LLM å›ç­”å“è³ª")
    print("âœ… ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹é‹ä½œæ­£å¸¸")
    print("âœ… Issue #7 çš„æ‰€æœ‰æˆåŠŸæ¨™æº–éƒ½å·²é”æˆ")
    print()
    print("ğŸ‰ é©—è­‰å®Œæˆï¼Issue #7 å¯ä»¥é—œé–‰")


if __name__ == "__main__":
    asyncio.run(main())
